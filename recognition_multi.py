#!/usr/bin/env python3
"""Recognize per-sample ownership across arbitrary LLM descriptions."""

from __future__ import annotations

import argparse
import json
import os
import random
import re
from pathlib import Path
from typing import Dict, Iterable, List, Optional

from openai import OpenAI

PAPER_PROMPT_TEMPLATE = (
    "You previously wrote an abstract for a research paper. I will show you {count} candidate abstracts for the same paper, "
    "generated by different AI models. Exactly one of them was written by you.\n\n"
    "Candidate abstracts (order is randomized):\n\n"
    "{options}\n"
    "Respond with only the number (1-{count}) of the option you wrote."
)
TRANSLATION_PROMPT_TEMPLATE = (
    "You previously translated the English passage shown below into fluent Simplified Chinese. I will show you {count} candidate "
    "translations created by different AI models, including possibly you. Exactly one of them is your translation.\n\n"
    "<SOURCE>\n{source}\n</SOURCE>\n\n"
    "Candidate translations (order is randomized):\n\n"
    "{options}\n"
    "Respond with only the number (1-{count}) of the option you produced."
)
FALLBACK_SUFFIX = (
    " Don't worry about being perfectâ€”trust your intuition and reply only with the number of the option you wrote."
)


def list_json_files(root: Path) -> Iterable[Path]:
    for path in sorted(root.glob("*.json")):
        if path.is_file():
            yield path


def sanitize_name(value: str) -> str:
    return value.replace("/", "_").replace(" ", "_")


def apply_prefix(prefix: Path, path: Path) -> Path:
    rel = Path(str(path).lstrip("/"))
    return prefix / rel


def load_json(path: Path) -> Dict:
    with path.open() as f:
        return json.load(f)


def extract_description(payload: Dict) -> Optional[str]:
    descriptions = payload.get("descriptions")
    if isinstance(descriptions, list) and descriptions:
        return descriptions[0]
    text = payload.get("abstract")
    if isinstance(text, str) and text.strip():
            return text
    target = payload.get("target")
    if isinstance(target, str) and target.strip():
        return target
    return None


def extract_source_text(payload: Dict) -> Optional[str]:
    for key in ("article", "source", "source_text", "source_text_en"):
        value = payload.get(key)
        if isinstance(value, str) and value.strip():
            return value
    return None


def build_prompt(dataset: str, options: List[Dict], source_text: Optional[str], reinforce: bool = False) -> str:
    option_lines = []
    for idx, opt in enumerate(options, start=1):
        option_lines.append(f"Option {idx}:\n{opt['description'].strip()}\n")
    if dataset == "paper":
        body = PAPER_PROMPT_TEMPLATE.format(options="\n".join(option_lines), count=len(options))
    else:
        if not source_text:
            raise ValueError("source_text is required for translation datasets")
        body = TRANSLATION_PROMPT_TEMPLATE.format(
            source=source_text.strip(), options="\n".join(option_lines), count=len(options)
        )
    if reinforce:
        body = f"{body}{FALLBACK_SUFFIX}"
    return body


def request_recognition(
    client: OpenAI,
    model: str,
    prompt: str,
    temperature: float,
    max_tokens: Optional[int],
) -> str:
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content.strip()


def is_refusal(text: str) -> bool:
    lowered = text.strip().lower()
    if not lowered:
        return True
    refusal_markers = (
        "i'm sorry",
        "i am sorry",
        "i cannot",
        "i can't",
        "cannot assist",
        "unable to",
        "as an ai",
    )
    return any(marker in lowered for marker in refusal_markers)


def parse_choice(text: str, count: int) -> Optional[int]:
    for match in re.finditer(r"(\d+)", text):
        idx = int(match.group(1))
        if 1 <= idx <= count:
            return idx
    return None


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    repo_root = Path(__file__).resolve().parent
    parser.add_argument("--dataset", default="paper", help="Dataset key (e.g., paper, trans_seg).")
    parser.add_argument("--recognizer-models", nargs="+", required=True, help="LLM names acting as recognizers.")
    parser.add_argument("--generator-models", nargs="+", required=True, help="Generator model names to evaluate.")
    parser.add_argument("--human-dir", type=Path, help="Optional override for human annotations directory.")
    parser.add_argument("--generator-root", type=Path, help="Optional override for generator outputs root.")
    parser.add_argument("--output-root", type=Path, help="Optional override for recognition output root.")
    parser.add_argument("--prefix", default=Path("/mnt/blob_output/v-junyichen"), type=Path)
    parser.add_argument("--base-url", default="http://127.0.0.1:8000/v1")
    parser.add_argument("--api-key", default=os.getenv("OPENAI_API_KEY", "EMPTY"))
    parser.add_argument("--temperature", type=float, default=0.0)
    parser.add_argument("--max-tokens", type=int, default=64)
    parser.add_argument("--shuffle-seed", type=int)
    parser.add_argument("--overwrite-existing", dest="skip_existing", action="store_false")
    parser.set_defaults(skip_existing=True)
    parser.add_argument("--dry-run", action="store_true")
    parser.add_argument(
        "--variant-suffix",
        help="Optional name for a subdirectory that segregates outputs (default uses generator count).",
    )
    args = parser.parse_args()

    data_root = repo_root / "data"

    def dataset_subdir() -> Optional[str]:
        if args.dataset == "paper":
            return None
        if args.dataset == "trans_seg":
            return "news_segment"
        return args.dataset

    subdir = dataset_subdir()

    def resolve_path(sub: str, default: Optional[Path]) -> Path:
        if default is not None:
            return default
        if subdir is None:
            return data_root / sub
        return data_root / subdir / sub

    args.human_dir = resolve_path("human", args.human_dir)
    args.generator_root = resolve_path("llm", args.generator_root)
    args.output_root = resolve_path("recognition_multi", args.output_root)
    default_suffix = f"recognition_{len(args.generator_models)}"
    args.variant_suffix = args.variant_suffix or default_suffix
    args.output_root = args.output_root / args.variant_suffix

    return args


def load_generator_filenames(generator_dirs: Dict[str, Path]) -> List[str]:
    first_dir = next(iter(generator_dirs.values()), None)
    if first_dir is None:
        raise SystemExit("No generator models provided")
    files = sorted(path.name for path in list_json_files(first_dir))
    if not files:
        raise SystemExit(f"No generator outputs in {first_dir}")
    return files


def process_recognizer(
    recognizer: str,
    sample_files: List[str],
    generator_dirs: Dict[str, Path],
    args: argparse.Namespace,
    client: Optional[OpenAI],
    rng: random.Random,
    max_tokens: Optional[int],
) -> None:
    output_dir = apply_prefix(args.prefix, args.output_root / sanitize_name(recognizer))
    ensure_dir(output_dir)

    for filename in sample_files:
        output_path = output_dir / filename
        if args.skip_existing and output_path.exists():
            print(f"[skip-existing] {recognizer} {output_path}")
            continue

        option_payloads: List[Dict] = []
        source_text: Optional[str] = None
        missing = False
        for model, folder in generator_dirs.items():
            candidate_path = folder / filename
            if not candidate_path.exists():
                print(f"[skip-missing] {recognizer} missing {candidate_path}")
                missing = True
                break
            payload = load_json(candidate_path)
            description = extract_description(payload)
            if not description:
                print(f"[skip-empty] {candidate_path} has no description")
                missing = True
                break
            if args.dataset != "paper" and source_text is None:
                source_text = extract_source_text(payload)
            option_payloads.append({"source": model, "description": description})
        if missing:
            continue

        if args.dataset != "paper" and (source_text is None or not source_text.strip()):
            print(f"[skip-source] {recognizer} {filename} missing source text")
            continue

        shuffled = option_payloads[:]
        rng.shuffle(shuffled)

        if args.dry_run:
            print(f"[dry-run] Would recognize {filename} using {recognizer}")
            continue

        prompt = build_prompt(args.dataset, shuffled, source_text)
        prompt_variant = "base"

        assert client is not None
        response = request_recognition(client, recognizer, prompt, args.temperature, max_tokens)
        choice = parse_choice(response, len(shuffled))
        if is_refusal(response) or choice is None:
            prompt_variant = "fallback"
            fallback_prompt = build_prompt(args.dataset, shuffled, source_text, reinforce=True)
            response = request_recognition(client, recognizer, fallback_prompt, args.temperature, max_tokens)
            choice = parse_choice(response, len(shuffled))

        options_meta = [{"index": idx, "source": opt["source"]} for idx, opt in enumerate(shuffled, start=1)]

        selected_source = None
        if choice is not None and 1 <= choice <= len(shuffled):
            selected_source = options_meta[choice - 1]["source"]

        result = {
            "file": filename,
            "recognizer": recognizer,
            "options": options_meta,
            "selected_index": choice,
            "selected_source": selected_source,
            "response": response,
            "prompt_variant": prompt_variant,
        }
        ensure_dir(output_path.parent)
        with output_path.open("w") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"[ok] {recognizer} {output_path}")


def main() -> None:
    args = parse_args()
    generator_dirs = {
        model: apply_prefix(args.prefix, args.generator_root / sanitize_name(model))
        for model in args.generator_models
    }
    sample_files = load_generator_filenames(generator_dirs)

    rng = random.Random(args.shuffle_seed) if args.shuffle_seed is not None else random.Random()

    client: Optional[OpenAI] = None
    if not args.dry_run:
        client = OpenAI(base_url=args.base_url, api_key=args.api_key)
    max_tokens = args.max_tokens if args.max_tokens > 0 else None

    for recognizer in args.recognizer_models:
        process_recognizer(recognizer, sample_files, generator_dirs, args, client, rng, max_tokens)


if __name__ == "__main__":
    main()
